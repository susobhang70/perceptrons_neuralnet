# Perceptron Algorithms & Neural Net

Perceptron based Linear Discriminant Functions and Simple Supervised Feed Forward Neural Network (with Backprop) implementation

## Perceptron LDFs

Run: `python perceptron_ldf.py`

The following algorithms are implemented:
1. `Single-sample perceptron`  
2. `Single-sample perceptron with margin`  
3. `Relaxation algorithm with margin`  
4. `Widrow-Hoff or Least Mean Squared (LMS) Rule`  

A graph is plotted showing the set of linearly separable points and the separation boundary generated by each algorithm. The weight vectors learned and also the time taken by each algorithm is output. The various factors like initial weight vector, learning rate, theta, margin, etc. can be tweaked and the results of the various algorithms can be seen.

Features/Observations:  
1. In each case, the data points are plotted in a graph (e.g. red: class- omega1 and blue: class- omega2) and the weight vector learnt from all of the above algorithms is shown in the same graph (labelled clearly to distinguish different solutions).
2. In case of Widrow Hoff/LMS, extra points were added to this data for making it linearly non-separable and it is seen that
LMS provides an acceptable decision boundary with some classification error. For this run as `python widrow_hoff.py`. Again the weight vectors in both cases (with and without the extra points) is plotted along with the points to show the same.

## Neural Network

Run: `python neuralnet.py`

A **simple supervised, feed-forward, back-propagation network** has been implemented with **sigmoid activation function** for the problem of **optical character recognition** for any three digits between 0 and 9.

- **Data:** Used any three digits between 0 and 9 from the optdigits data set that comes from the `UCI Machine Learning Repository`. (training and cross validation files have been included)
- **Pre-processing:** Down-sampled images (to 8x8), and then to 1x64 (1x65 actually with 1 bias term) one dimensional matrix to feed to NN
- **Classifier:** 20 units in the Hidden layer and 2 units in the Output layer have been used here. A variant with 8 units in Hidden Layer is also implemented.
- **Result:** The number of correctly classified samples from the cross validation file is output as a percentage.
